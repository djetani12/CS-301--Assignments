The code done for this assignement is an implemantion of a Multi Layered Perceptron(MLP) neural network that uses JAX, instead of numpy. More specifiicaly it is done with the JAX autodiff library, one that I set using "import jax.numpy as jnp". A MLP is a series of layers that contains nuerons. The actual neurons consist the weight and biases that get learned through training. It is trained through the use of a dataset of input to output pairs and the whole goal is to find out how to map between these.
The starts with defining the "Value" class which includes the input part. It also includes the graidents during the backpropagation process. The "Value" class also contains defintion to different arthmitic operations and activiation functions like "tanh", which I used instead of ReLu since in the video provided I was told they were similar and not much difference in either method.
This leads to the next classes called "Neuron" and "Layer" which are of course used to build the actual MLP. The neuron class conists of defintion of one nueron which produces an output value. The layer class takes the input data and outputs a set of values.
The MLP class is defined to combine the nuerons and layers into the nueral netowrk, it brings it all together. It includes a forward pass that takes an input vector and provides osme value output. The MLP is also trained using the loss function which is the measurement of the difference in predicted and actual value. The loss function is minimized using gradient descent algorithm, that bascially updates the weight and bias of the neuron so it can improve the accuract in prediction of the values.
The example is used from the demo provided in the github repositry that the author uses. It is a copy and paste, only changes are the ones that change numpy into jax.
